{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhB3Ie54m51p",
    "outputId": "90ce5b16-df57-4240-ffcf-dededba103b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 19 09:26:20 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:73:00.0  On |                  Off |\n",
      "| 30%   41C    P8    29W / 230W |  22663MiB / 24564MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQgn4G1UnI-1",
    "outputId": "bac45586-f58c-495a-ef74-7c14fb706077"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_0cwLyDm9MA",
    "outputId": "bf921d89-59a8-4a68-a334-ec60009ca3ef"
   },
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir /root/.kaggle\n",
    "\n",
    "# !cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
    "# !chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sBsX94Cngmw",
    "outputId": "5abbc75e-d911-4eaf-ab06-21d73ea3ff63"
   },
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d chaudharypriyanshu/bci-42a-dataset\n",
    "# ! unzip /content/bci-42a-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pNuGngW5Ry4g"
   },
   "outputs": [],
   "source": [
    "# !pip install neural_structured_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "5vqqbcpouQfP"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  results_path=\"/workspace/SAMAD_PRAGATI_PRATYUSH_KARINA/EEG-ATCNet/results/\"\n",
    "#   data_path='/kaggle/input/bci-42a-dataset/'\n",
    "  data_path=\"/workspace/SAMAD_PRAGATI_PRATYUSH_KARINA/EEG-ATCNet/BCICIV_2a_gdf/\"\n",
    "  n_classes=4\n",
    "  n_sub=9\n",
    "  n_channels=22\n",
    "  isStandard=True\n",
    "  LOSO=False\n",
    "  verbose=0\n",
    "  batch_size=64\n",
    "  epochs=1000\n",
    "  patience=300\n",
    "  lr=0.0007\n",
    "  LearnCurves=True\n",
    "  n_train=1\n",
    "  model_name='ATCNet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0FMLQ4dhW4qE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "random.seed(1001)\n",
    "def channels(X_train):\n",
    "  train=pd.DataFrame(columns=range(0,22))\n",
    "  for i in tqdm(range(0,X_train.shape[0])):\n",
    "    t=X_train[i,0]\n",
    "    corr=np.corrcoef(t)\n",
    "    train=pd.concat([train,pd.DataFrame(corr,columns=range(0,22))],axis=0)\n",
    "  kmeans = KMeans(n_clusters=3,tol=0,random_state=0,algorithm='elkan').fit(train.T.values)\n",
    "  channellabels=kmeans.labels_ \n",
    "  return [1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2] \n",
    "\n",
    "def augment(X_train,y_train,counts=0):\n",
    "  data=[]\n",
    "  labels=[]\n",
    "  for i in [1,2,3,4]:\n",
    "    result = np.where(y_train == i)[0]\n",
    "    temp=np.copy(X_train[result,:,:,:])\n",
    "    print('result:',temp.shape)\n",
    "    channellabels=channels(temp)\n",
    "    done=set()\n",
    "    for turn in range(counts):\n",
    "      clst=random.randint(0,2)\n",
    "      cluster=np.where(channellabels==clst)[0]\n",
    "      r1=random.randint(0, 71)\n",
    "      r2=random.randint(0, 71)\n",
    "      if((r1==r2) or ((r1,r2,clst) in done)):\n",
    "        continue\n",
    "      done.add((r1,r2,clst))\n",
    "\n",
    "      transform1=np.copy(temp[r1,:,:,:])\n",
    "      transform2=np.copy(temp[r2,:,:,:])\n",
    "\n",
    "      transform1[:,cluster,:]=transform2[:,cluster,:]\n",
    "      data.append(transform1)\n",
    "      labels.append(i)\n",
    "\n",
    "  return data,labels\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "#%%\n",
    "def load_data_LOSO (data_path, subject): \n",
    "\n",
    "    X_train, y_train = [], []\n",
    "    for sub in range (0,9):\n",
    "        path = data_path\n",
    "        \n",
    "        X1, y1 = load_data(sub+1, True ,path)\n",
    "        X2, y2 = load_data(sub+1, False ,path)\n",
    "        X = np.concatenate((X1, X2), axis=0)\n",
    "        y = np.concatenate((y1, y2), axis=0)\n",
    "                   \n",
    "        if (sub == subject):\n",
    "            X_test = X\n",
    "            y_test = y\n",
    "        elif (X_train == []):\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X), axis=0)\n",
    "            y_train = np.concatenate((y_train, y), axis=0)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "#%%\n",
    "def load_data(data_path, subject, training, all_trials = True):\n",
    "\n",
    "\tn_channels = 22\n",
    "\tn_tests = 6*48 \t\n",
    "\twindow_Length = 7*250 \n",
    "\n",
    "\tclass_return = np.zeros(n_tests)\n",
    "\tdata_return = np.zeros((n_tests, n_channels, window_Length))\n",
    "\n",
    "\tNO_valid_trial = 0\n",
    "\tif training:\n",
    "\t\ta = sio.loadmat(data_path+'A0'+str(subject)+'T.mat')\n",
    "\telse:\n",
    "\t\ta = sio.loadmat(data_path+'A0'+str(subject)+'E.mat')\n",
    "\ta_data = a['data']\n",
    "\tfor ii in range(0,a_data.size):\n",
    "\t\ta_data1 = a_data[0,ii]\n",
    "\t\ta_data2= [a_data1[0,0]]\n",
    "\t\ta_data3= a_data2[0]\n",
    "\t\ta_X \t\t= a_data3[0]\n",
    "\t\ta_trial \t= a_data3[1]\n",
    "\t\ta_y \t\t= a_data3[2]\n",
    "\t\ta_artifacts = a_data3[5]\n",
    "\n",
    "\t\tfor trial in range(0,a_trial.size):\n",
    " \t\t\tif(a_artifacts[trial] != 0 and not all_trials):\n",
    " \t\t\t    continue\n",
    "\n",
    " \t\t\tdata_return[NO_valid_trial,:,:] = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial])+window_Length),:22])\n",
    " \t\t\tclass_return[NO_valid_trial] = int(a_y[trial])\n",
    " \t\t\tNO_valid_trial +=1\n",
    "\n",
    "\treturn data_return[0:NO_valid_trial,:,:], class_return[0:NO_valid_trial]\n",
    "\n",
    "#%%\n",
    "def standardize_data(X_train, X_test, channels): \n",
    "\n",
    "    for j in range(channels):\n",
    "          scaler = StandardScaler()\n",
    "          scaler.fit(X_train[:,0, j, :])\n",
    "          X_train[:,0, j, :] = scaler.transform(X_train[:,0, j, :])\n",
    "          X_test[:,0, j, :] = scaler.transform(X_test[:,0, j, :])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "#%%\n",
    "def get_data(path, subject, LOSO = False, isStandard = True):\n",
    "\n",
    "    fs = 250  \n",
    "    t1 = int(1.5*fs) \n",
    "    t2 = int(6*fs)   \n",
    "    T = t2-t1        \n",
    "    \n",
    "    if LOSO:\n",
    "\n",
    "        X_train, y_train, X_test, y_test = load_data_LOSO(path, subject)\n",
    "    else:\n",
    "  \n",
    "        path = path\n",
    "        X_train, y_train = load_data(path, subject+1, True)\n",
    "        X_test, y_test = load_data(path, subject+1, False)\n",
    "\n",
    "    N_tr, N_ch, _ = X_train.shape \n",
    "    X_train = X_train[:, :, t1:t2].reshape(N_tr,1, N_ch, T)\n",
    "\n",
    "    X_train=CAR(X_train)\n",
    "    \n",
    "    data,label=augment(X_train,y_train,40)\n",
    "    X_train=np.concatenate((X_train,data),axis=0)\n",
    "    y_train=np.concatenate((y_train,label),axis=0)\n",
    "\n",
    "    y_train_onehot = (y_train-1).astype(int)\n",
    "    y_train_onehot = to_categorical(y_train_onehot)\n",
    " \n",
    "    N_test, N_ch, _ = X_test.shape \n",
    "    X_test = X_test[:, :, t1:t2].reshape(N_test,1, N_ch, T)\n",
    "    y_test_onehot = (y_test-1).astype(int)\n",
    "    y_test_onehot = to_categorical(y_test_onehot)\t\n",
    "    X_test=CAR(X_test)\n",
    "\n",
    "    print('TRAINING_SHAPE:',X_train.shape)\n",
    "\n",
    "\n",
    "    if (isStandard == True):\n",
    "        X_train, X_test = standardize_data(X_train, X_test, N_ch)\n",
    "\n",
    "\n",
    "    return X_train, y_train, y_train_onehot, X_test, y_test, y_test_onehot\n",
    "\n",
    "def CAR(trials):\n",
    "    sigma = np.mean(trials, axis=2, keepdims=True)\n",
    "    tnew = (trials - sigma)\n",
    "    return tnew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "H71muKsq6cxL"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense\n",
    "from tensorflow.keras.layers import multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from tensorflow.keras.layers import Dropout, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense\n",
    "from tensorflow.keras.layers import multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from tensorflow.keras.layers import Dropout, MultiHeadAttention, LayerNormalization,SeparableConv1D,AveragePooling1D,MaxPool1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.nn import avg_pool1d\n",
    "\n",
    "#%% Creat and return the attention model\n",
    "def get_attention_model(net, attention_model): \n",
    "    expanded_axis = 3 # defualt = 3\n",
    "    if attention_model == 'mha':   # Multi-head self attention layer \n",
    "        net = mha_block(net)\n",
    "    elif attention_model == 'mhla':  # Multi-head local self-attention layer \n",
    "        net = mha_block(net, vanilla = False)\n",
    "    elif attention_model == 'se':   # Squeeze-and-excitation layer\n",
    "        if(len(net.shape) < 4):\n",
    "            net = tf.expand_dims(net, axis=expanded_axis)\n",
    "        net = se_block(net, ratio=8)\n",
    "    elif attention_model == 'cbam': # Convolutional block attention module\n",
    "        if(len(net.shape) < 4):\n",
    "            net = tf.expand_dims(net, axis=expanded_axis)\n",
    "        net = cbam_block(net, ratio=8)\n",
    "    else:\n",
    "        raise Exception(\"'{}' is not supported attention module!\".format(attention_model))\n",
    "        \n",
    "    if(len(net.shape) == 4):\n",
    "        net = K.squeeze(net, expanded_axis)\n",
    "    return net\n",
    "\n",
    "\n",
    "#%% Multi-head self Attention (MHA) block\n",
    "def mha_block(input_feature, key_dim=8, num_heads=2, dropout = 0.5, vanilla = True):\n",
    "    \"\"\"Multi Head self Attention (MHA) block.     \n",
    "       \n",
    "    Here we include two types of MHA blocks: \n",
    "            The original multi-head self-attention as described in https://arxiv.org/abs/1706.03762\n",
    "            The multi-head local self attention as described in https://arxiv.org/abs/2112.13492v1\n",
    "    \"\"\"    \n",
    "    # Layer normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(input_feature)\n",
    "    \n",
    "    if vanilla:\n",
    "        # Create a multi-head attention layer as described in \n",
    "        # 'Attention Is All You Need' https://arxiv.org/abs/1706.03762\n",
    "        x = MultiHeadAttention(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(x, x)\n",
    "    else:\n",
    "        # Create a multi-head local self-attention layer as described in \n",
    "        # 'Vision Transformer for Small-Size Datasets' https://arxiv.org/abs/2112.13492v1\n",
    "        \n",
    "        # Build the diagonal attention mask\n",
    "        NUM_PATCHES = input_feature.shape[1]\n",
    "        diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "        diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n",
    "        \n",
    "        # Create a multi-head local self attention layer.\n",
    "        x = MultiHeadAttention_LSA(key_dim = key_dim, num_heads = num_heads, dropout = dropout)(\n",
    "            x, x, attention_mask = diag_attn_mask)\n",
    "        \n",
    "    x = Dropout(0.3)(x)\n",
    "    # Skip connection\n",
    "    mha_feature = Add()([input_feature, x])\n",
    "    \n",
    "    return mha_feature\n",
    "\n",
    "\n",
    "#%% Multi head self Attention (MHA) block: Locality Self Attention (LSA)\n",
    "class MultiHeadAttention_LSA(tf.keras.layers.MultiHeadAttention):\n",
    "    \"\"\"local multi-head self attention block\n",
    "     \n",
    "     Locality Self Attention as described in https://arxiv.org/abs/2112.13492v1\n",
    "     This implementation is taken from  https://keras.io/examples/vision/vit_small_ds/ \n",
    "    \"\"\"    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is the square \n",
    "        # root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores\n",
    "\n",
    "\n",
    "#%% Squeeze-and-excitation block\n",
    "def se_block(input_feature, ratio=8):\n",
    "\t\"\"\"Squeeze-and-Excitation(SE) block.\n",
    "    \n",
    "\tAs described in https://arxiv.org/abs/1709.01507\n",
    "    The implementation is taken from https://github.com/kobiso/CBAM-keras\n",
    "\t\"\"\"\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\n",
    "\tse_feature = GlobalAveragePooling2D()(input_feature)\n",
    "\tse_feature = Reshape((1, 1, channel))(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tse_feature = Dense(channel // ratio,\n",
    "\t\t\t\t\t   activation='relu',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel//ratio)\n",
    "\tse_feature = Dense(channel,\n",
    "\t\t\t\t\t   activation='sigmoid',\n",
    "\t\t\t\t\t   kernel_initializer='he_normal',\n",
    "\t\t\t\t\t   use_bias=True,\n",
    "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
    "\tassert se_feature.shape[1:] == (1,1,channel)\n",
    "\tif K.image_data_format() == 'channels_first':\n",
    "\t\tse_feature = Permute((3, 1, 2))(se_feature)\n",
    "\n",
    "\tse_feature = multiply([input_feature, se_feature])\n",
    "\treturn se_feature\n",
    "\n",
    "\n",
    "#%% Convolutional block attention module\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "\t\"\"\" Convolutional Block Attention Module(CBAM) block.\n",
    "    \n",
    "\tAs described in https://arxiv.org/abs/1807.06521\n",
    "    The implementation is taken from https://github.com/kobiso/CBAM-keras\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature)\n",
    "\treturn cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "# \tchannel = input_feature._keras_shape[channel_axis]\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "    \n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\t\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\t\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tactivation='sigmoid',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\t\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xKEO2iDdbXfz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, AveragePooling2D\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LayerNormalization, Flatten \n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda, Input, Permute\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "def ATCNet(n_classes, in_chans = 22, in_samples = 1000, n_windows = 3, attention = None, \n",
    "           eegn_F1 = 16, eegn_D = 2, eegn_kernelSize = 64, eegn_poolSize = 8, eegn_dropout=0.3, \n",
    "           tcn_depth = 2, tcn_kernelSize = 4, tcn_filters = 32, tcn_dropout = 0.3, \n",
    "           tcn_activation = 'elu', fuse = 'average'):\n",
    "\n",
    "    input_1 = Input(shape = (1,in_chans, in_samples))\n",
    "    input_2 = Permute((3,2,1))(input_1) \n",
    "    regRate=.25\n",
    "    numFilters = eegn_F1\n",
    "    F2 = numFilters*eegn_D\n",
    "\n",
    "    block1 = Conv_block(input_layer = input_2, F1 = eegn_F1, D = eegn_D, poolSize = eegn_poolSize, Filters=[16,32,64],\n",
    "                        in_chans = in_chans, dropout = eegn_dropout)\n",
    "    block1 = Lambda(lambda x: x[:,:,-1,:])(block1)\n",
    "    print('ATCNet block1 shape:',block1.shape)\n",
    "\n",
    "    sw_concat = []  \n",
    "    embed = []\n",
    "    for i in range(n_windows):\n",
    "        st = i\n",
    "        end = block1.shape[1]-n_windows+i+1\n",
    "        block2 = block1[:, st:end, :]\n",
    "        \n",
    "        if attention is not None:\n",
    "            block2 = get_attention_model(block2, attention)\n",
    "\n",
    "        block3 = TCN_block(input_layer = block2, input_dimension = F2, depth = tcn_depth,\n",
    "                            kernel_size = tcn_kernelSize, filters = tcn_filters, \n",
    "                            dropout = tcn_dropout, activation = tcn_activation)\n",
    "\n",
    "        embed.append(block3)\n",
    "        \n",
    "        block3 = Lambda(lambda x: x[:,-1,:])(block3)\n",
    "\n",
    "        if(fuse == 'average'):\n",
    "            sw_concat.append(Dense(n_classes, kernel_constraint = max_norm(regRate))(block3))\n",
    "        elif(fuse == 'concat'):\n",
    "            if i == 0:\n",
    "                sw_concat = block3\n",
    "            else:\n",
    "                sw_concat = Concatenate()([sw_concat, block3])\n",
    "                \n",
    "    if(fuse == 'average'):\n",
    "        if len(sw_concat) > 1:\n",
    "            sw_concat = tf.keras.layers.Average()(sw_concat[:])\n",
    "        else:\n",
    "            sw_concat = sw_concat[0]\n",
    "    elif(fuse == 'concat'):\n",
    "        sw_concat = Dense(n_classes, kernel_constraint = max_norm(regRate))(sw_concat)\n",
    "            \n",
    "    embed = Concatenate(name = 'concit')(embed)\n",
    "    softmax = Activation('softmax', name = 'softmax')(sw_concat)\n",
    "    \n",
    "    return Model(inputs = input_1, outputs = [softmax,embed])\n",
    "\n",
    "\n",
    "def Conv_block(input_layer, F1=4, Filters=[1,2,4,8,16,32,64], poolSize=7, D=2, in_chans=22, dropout=0.1):\n",
    "\n",
    "    F2= F1*D\n",
    "    out_sep=[]\n",
    "    for i,kernel_size in enumerate(Filters):\n",
    "      block1 = Conv2D(8, (kernel_size,1), padding = 'same',data_format='channels_last',use_bias = False)(input_layer)\n",
    "      out_sep.append(block1)\n",
    "      print('kernel_size:',block1.shape)\n",
    "\n",
    "    block1=Concatenate(axis=-1)(out_sep)\n",
    "    block1 = BatchNormalization(axis = -1)(block1)\n",
    "\n",
    "\n",
    "    print('block1:',block1.shape)\n",
    "    block2 = DepthwiseConv2D((1, in_chans), use_bias = False, \n",
    "                                    depth_multiplier = D,\n",
    "                                    data_format='channels_last',\n",
    "                                    depthwise_constraint = max_norm(1.))(block1)\n",
    "    block2 = BatchNormalization(axis = -1)(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    print('block2:',block2.shape)\n",
    "    block2 = AveragePooling2D((7,1),data_format='channels_last')(block2)\n",
    "    block2 = Dropout(dropout)(block2)\n",
    "    print('After pooling block2:',block2.shape)\n",
    "    block3 = Conv2D(F2,(16,1),\n",
    "                            data_format='channels_last',\n",
    "                            use_bias = False, padding = 'same')(block2)\n",
    "    block3 = BatchNormalization(axis = -1)(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    print('block3:',block3.shape)\n",
    "    block3 = AveragePooling2D((poolSize,1),data_format='channels_last')(block3)\n",
    "    block3 = Dropout(dropout)(block3)\n",
    "    print('block3:',block3.shape)\n",
    "    return block3\n",
    "\n",
    "def TCN_block(input_layer,input_dimension,depth,kernel_size,filters,dropout,activation='relu'):\n",
    "    \n",
    "    block = SeparableConv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(input_layer)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation(activation)(block)\n",
    "    block = Dropout(dropout)(block)\n",
    "    block = SeparableConv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation(activation)(block)\n",
    "    block = Dropout(dropout)(block)\n",
    "    if(input_dimension != filters):\n",
    "        conv = SeparableConv1D(filters,kernel_size=1,padding='same')(input_layer)\n",
    "        added = Add()([block,conv])\n",
    "    else:\n",
    "        added = Add()([block,input_layer])\n",
    "    out = Activation(activation)(added)\n",
    "    \n",
    "    for i in range(depth-1):\n",
    "        block = SeparableConv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(out)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation(activation)(block)\n",
    "        block = Dropout(dropout)(block)\n",
    "        block = SeparableConv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(block)\n",
    "        block = BatchNormalization()(block)\n",
    "        block = Activation(activation)(block)\n",
    "        block = Dropout(dropout)(block)      \n",
    "        added = Add()([block, out])\n",
    "        out = Activation(activation)(added)\n",
    "      \n",
    "\n",
    "    return out\n",
    "\n",
    "def  Xception_block(input_layer,filters=8,layers=[8,16,32],dilation_rate=1,dropout=0.1,activation='relu'):\n",
    "    \n",
    "    print('Xception input:',input_layer.shape)\n",
    "    block1= Conv1D(filters,kernel_size=4,dilation_rate=dilation_rate,activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(input_layer)\n",
    "    output=[]\n",
    "    for _,kernel in enumerate(layers):\n",
    "          out_sep=tf.keras.layers.SeparableConv1D(filters,kernel,padding='causal',dilation_rate=dilation_rate)(block1)\n",
    "          out_sep = BatchNormalization()(out_sep)\n",
    "          output.append(out_sep)\n",
    "    output=Concatenate(axis=-1)(output)\n",
    "    output = Activation(activation)(output) \n",
    "\n",
    "    print('Xception parallel separable conv shape:',output.shape)\n",
    "    block2=Conv1D(filters,kernel_size=5,dilation_rate=dilation_rate,activation='linear',\n",
    "                   padding = 'causal',kernel_initializer='he_uniform')(input_layer)  \n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation(activation)(block2)                             \n",
    "    print('Xception block2 shape:',block2.shape)\n",
    "    output=Concatenate(axis=-1)([output,block2])\n",
    "    print('Xception output shape:',output.shape)  \n",
    "    return output\n",
    "\n",
    "def TCNet_Fusion(n_classes,Chans=22, Samples=1000, layers=2, kernel_s=4, filt=12,\n",
    "                 dropout=0.3, activation='elu', F1=24, D=2, kernLength=32, dropout_eeg=0.3):\n",
    "\n",
    "    input1 = Input(shape = (1,Chans, Samples))\n",
    "    input2 = Permute((3,2,1))(input1)\n",
    "    regRate=.25\n",
    "\n",
    "    numFilters = F1\n",
    "    F2= numFilters*D\n",
    "    \n",
    "    EEGNet_sep = EEGNet(input_layer=input2,F1=F1,kernLength=kernLength,D=D,Chans=Chans,dropout=dropout_eeg)\n",
    "    block2 = Lambda(lambda x: x[:,:,-1,:])(EEGNet_sep)\n",
    "    FC = Flatten()(block2) \n",
    "\n",
    "    outs = TCN_block(input_layer=block2,input_dimension=F2,depth=layers,kernel_size=kernel_s,filters=filt,dropout=dropout,activation=activation)\n",
    "\n",
    "    Con1 = Concatenate()([block2,outs]) \n",
    "    out = Flatten()(Con1) \n",
    "    Con2 = Concatenate()([out,FC]) \n",
    "    dense        = Dense(n_classes, name = 'dense',kernel_constraint = max_norm(regRate))(Con2)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1,outputs=softmax)\n",
    "\n",
    "\n",
    "def EEGTCNet(n_classes, Chans=22, Samples=1000, layers=2, kernel_s=4, filt=12, dropout=0.3, activation='elu', F1=8, D=2, kernLength=32, dropout_eeg=0.2):\n",
    "\n",
    "    input1 = Input(shape = (1,Chans, Samples))\n",
    "    input2 = Permute((3,2,1))(input1)\n",
    "    regRate=.25\n",
    "    numFilters = F1\n",
    "    F2= numFilters*D\n",
    "\n",
    "    EEGNet_sep = EEGNet(input_layer=input2,F1=F1,kernLength=kernLength,D=D,Chans=Chans,dropout=dropout_eeg)\n",
    "    block2 = Lambda(lambda x: x[:,:,-1,:])(EEGNet_sep)\n",
    "    outs = TCN_block(input_layer=block2,input_dimension=F2,depth=layers,kernel_size=kernel_s,filters=filt,dropout=dropout,activation=activation)\n",
    "    out = Lambda(lambda x: x[:,-1,:])(outs)\n",
    "    dense        = Dense(n_classes, name = 'dense',kernel_constraint = max_norm(regRate))(out)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1,outputs=softmax)\n",
    "\n",
    "def EEGNeX_8_32(n_timesteps, n_features, n_outputs):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1, n_features, n_timesteps)))\n",
    "\n",
    "    model.add(Conv2D(filters=8, kernel_size=(1, 32), use_bias = False, padding='same', data_format=\"channels_first\"))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Activation(activation='elu'))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(1, 32), use_bias = False, padding='same', data_format=\"channels_first\"))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Activation(activation='elu'))\n",
    "\n",
    "    model.add(DepthwiseConv2D(kernel_size=(n_features, 1), depth_multiplier=2, use_bias = False, depthwise_constraint=max_norm(1.), data_format=\"channels_first\"))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Activation(activation='elu'))\n",
    "    model.add(AveragePooling2D(pool_size=(1, 4), padding='same', data_format=\"channels_first\"))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(1, 16), use_bias = False, padding='same', dilation_rate=(1, 2), data_format='channels_first'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Activation(activation='elu'))\n",
    "    \n",
    "    model.add(Conv2D(filters=8, kernel_size=(1, 16), use_bias = False, padding='same', dilation_rate=(1, 4),  data_format='channels_first'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Activation(activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_outputs, kernel_constraint=max_norm(0.25)))\n",
    "    model.add(Activation(activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "def EEGNet_classifier(n_classes, Chans=22, Samples=1000, F1=8, D=2, kernLength=64, dropout_eeg=0.25):\n",
    "    input1 = Input(shape = (1,Chans, Samples))   \n",
    "    input2 = Permute((3,2,1))(input1) \n",
    "    regRate=.25\n",
    "\n",
    "    eegnet = EEGNet(input_layer=input2, F1=F1, kernLength=kernLength, D=D, Chans=Chans, dropout=dropout_eeg)\n",
    "    eegnet = Flatten()(eegnet)\n",
    "    dense = Dense(n_classes, name = 'dense',kernel_constraint = max_norm(regRate))(eegnet)\n",
    "    softmax = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1, outputs=softmax)\n",
    "\n",
    "def EEGNet(input_layer, F1=8, kernLength=64, D=2, Chans=22, dropout=0.25):\n",
    "    F2= F1*D\n",
    "    block1 = Conv2D(F1, (kernLength, 1), padding = 'same',data_format='channels_last',use_bias = False)(input_layer)\n",
    "    block1 = BatchNormalization(axis = -1)(block1)\n",
    "    block2 = DepthwiseConv2D((1, Chans), use_bias = False, \n",
    "                                    depth_multiplier = D,\n",
    "                                    data_format='channels_last',\n",
    "                                    depthwise_constraint = max_norm(1.))(block1)\n",
    "    block2 = BatchNormalization(axis = -1)(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((8,1),data_format='channels_last')(block2)\n",
    "    block2 = Dropout(dropout)(block2)\n",
    "    block3 = SeparableConv2D(F2, (16, 1),\n",
    "                            data_format='channels_last',\n",
    "                            use_bias = False, padding = 'same')(block2)\n",
    "    block3 = BatchNormalization(axis = -1)(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "    block3 = AveragePooling2D((8,1),data_format='channels_last')(block3)\n",
    "    block3 = Dropout(dropout)(block3)\n",
    "    return block3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-EUR0f1bexA",
    "outputId": "22792d91-0601-4911-983c-ae1d10161de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_size: (None, 1125, 22, 8)\n",
      "kernel_size: (None, 1125, 22, 8)\n",
      "kernel_size: (None, 1125, 22, 8)\n",
      "block1: (None, 1125, 22, 24)\n",
      "block2: (None, 1125, 1, 48)\n",
      "After pooling block2: (None, 160, 1, 48)\n",
      "block3: (None, 160, 1, 32)\n",
      "block3: (None, 22, 1, 32)\n",
      "ATCNet block1 shape: (None, 22, 32)\n",
      "result: (72, 1, 22, 1125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:00<00:00, 1011.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: (72, 1, 22, 1125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:00<00:00, 1075.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: (72, 1, 22, 1125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:00<00:00, 789.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: (72, 1, 22, 1125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 72/72 [00:00<00:00, 1058.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING_SHAPE: (446, 1, 22, 1125)\n",
      "9/9 [==============================] - 1s 18ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'argmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 282\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [34], line 278\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m#train()\u001b[39;00m\n\u001b[1;32m    276\u001b[0m dataset_conf \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_sub\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m9\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_channels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m22\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m: CFG\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misStandard\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOSO\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[0;32m--> 278\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [34], line 190\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, dataset_conf, results_path, allRuns)\u001b[0m\n\u001b[1;32m    187\u001b[0m filepath \u001b[38;5;241m=\u001b[39m best_models\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m    188\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(results_path \u001b[38;5;241m+\u001b[39m filepath[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 190\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    192\u001b[0m labels \u001b[38;5;241m=\u001b[39m y_test_onehot\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    193\u001b[0m acc_bestRun[sub] \u001b[38;5;241m=\u001b[39m accuracy_score(labels, y_pred)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#%%\n",
    "def draw_learning_curves(history):\n",
    "    plt.plot(history.history['softmax_accuracy'])\n",
    "    plt.plot(history.history['val_softmax_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def draw_confusion_matrix(cf_matrix, sub, results_path):\n",
    "    # Generate confusion matrix plot\n",
    "    display_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix, \n",
    "                                display_labels=display_labels)\n",
    "    disp.plot()\n",
    "    disp.ax_.set_xticklabels(display_labels, rotation=12)\n",
    "    plt.title('Confusion Matrix of Subject: ' + sub )\n",
    "    plt.savefig(results_path + '/subject_' + sub + '.png')\n",
    "    plt.show()\n",
    "\n",
    "def draw_performance_barChart(num_sub, metric, label):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = list(range(1, num_sub+1))\n",
    "    ax.bar(x, metric, 0.5, label=label)\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_xlabel(\"Subject\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title('Model '+ label + ' per subject')\n",
    "    ax.set_ylim([0,1])\n",
    "    \n",
    "\n",
    "#%% Training \n",
    "def train():\n",
    "    in_exp=time.time()\n",
    "\n",
    "    best_models = open(CFG.results_path + \"/best models.txt\", \"w\")\n",
    "\n",
    "    log_write = open(CFG.results_path + \"/log.txt\", \"w\")\n",
    "\n",
    "    perf_allRuns = open(CFG.results_path + \"/perf_allRuns.npz\", 'wb')\n",
    "    \n",
    "    # Initialize variables\n",
    "    acc = np.zeros((CFG.n_sub, CFG.n_train))\n",
    "    kappa = np.zeros((CFG.n_sub, CFG.n_train))\n",
    "    \n",
    "    # Iteration over subjects\n",
    "    for sub in [0,3,8]: \n",
    "  \n",
    "        print('\\nTraining on subject ', sub+1)\n",
    "        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n",
    "\n",
    "        BestSubjAcc = 0 \n",
    "        bestTrainingHistory = [] \n",
    "\n",
    "        X_train, _, y_train_onehot, X_test, _, y_test_onehot = get_data(\n",
    "            CFG.data_path, sub, CFG.LOSO, CFG.isStandard)\n",
    "\n",
    "        in_sub = time.time()\n",
    "        for train in range(CFG.n_train): \n",
    "\n",
    "            in_run = time.time()\n",
    "\n",
    "            filepath = CFG.results_path + '/saved models/run-{}'.format(train+1)\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)        \n",
    "            filepath = filepath + '/subject-{}.h5'.format(sub+1)\n",
    "            \n",
    "            # Create the model\n",
    "            model = getModel(CFG.model_name)\n",
    "            opt=Adam(lr=CFG.lr)\n",
    "\n",
    "            model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['accuracy'],loss_weights=[1., 0.0])          \n",
    "            callbacks = [\n",
    "                ModelCheckpoint(filepath, monitor='val_softmax_accuracy', verbose=2, \n",
    "                                save_best_only=True, save_weights_only=True, mode='max'),\n",
    "                EarlyStopping(monitor='val_softmax_accuracy', verbose=0, mode='max', patience=CFG.patience+1000),\n",
    "            ]\n",
    "\n",
    "            val_data=(X_test,[y_test_onehot, np.zeros((len(y_test_onehot),18,160))])\n",
    "            history = model.fit(X_train,[y_train_onehot,np.zeros((len(y_train_onehot),18,160))], validation_data=val_data, \n",
    "                                epochs=CFG.epochs, batch_size=CFG.batch_size, callbacks=callbacks, verbose=0)\n",
    "\n",
    "            model.load_weights(filepath)\n",
    "            test={'input': X_test,'label':y_test_onehot}\n",
    "\n",
    "\n",
    "            embed_train = model.predict(X_train)[1]\n",
    "            embed_test = model.predict(X_test)[1]\n",
    "            \n",
    "            np.save(f'y_train_sub{sub}.npy',y_train_onehot)\n",
    "            np.save(f'embed_train_sub{sub}.npy',embed_train)\n",
    "            np.save(f'embed_test_sub{sub}.npy',embed_test)\n",
    "\n",
    "            y_pred = model.predict(X_test)[0].argmax(axis=-1)\n",
    "            labels = y_test_onehot.argmax(axis=-1)\n",
    "            acc[sub, train]  = accuracy_score(labels, y_pred)\n",
    "            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n",
    "\n",
    "            out_run = time.time()\n",
    "\n",
    "            info = 'Subject: {}   Train no. {}   Time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n",
    "            info = info + 'Test_acc: {:.4f}   Test_kappa: {:.4f}'.format(acc[sub, train], kappa[sub, train])\n",
    "            print(info)\n",
    "            log_write.write(info +'\\n')\n",
    "\n",
    "            if(BestSubjAcc < acc[sub, train]):\n",
    "                 BestSubjAcc = acc[sub, train]\n",
    "                 bestTrainingHistory = history\n",
    "\n",
    "        best_run = np.argmax(acc[sub,:])\n",
    "        filepath = '/saved models/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n",
    "        best_models.write(filepath)\n",
    "        out_sub = time.time()\n",
    "\n",
    "        info = '----------\\n'\n",
    "        info = info + 'Subject: {}   best_run: {}   Time: {:.1f} m   '.format(sub+1, best_run+1, ((out_sub-in_sub)/60))\n",
    "        info = info + 'acc: {:.4f}   avg_acc: {:.4f} +- {:.4f}   '.format(acc[sub, best_run], np.average(acc[sub, :]), acc[sub,:].std() )\n",
    "        info = info + 'kappa: {:.4f}   avg_kappa: {:.4f} +- {:.4f}'.format(kappa[sub, best_run], np.average(kappa[sub, :]), kappa[sub,:].std())\n",
    "        info = info + '\\n----------'\n",
    "        print(info)\n",
    "        log_write.write(info+'\\n')\n",
    "        if (CFG.LearnCurves):\n",
    "            print('Plot Learning Curves ....... ')\n",
    "            draw_learning_curves(bestTrainingHistory)\n",
    "\n",
    "    out_exp = time.time()\n",
    "    info = '\\nTime: {:.1f} h   '.format( (out_exp-in_exp)/(60*60) )\n",
    "    print(info)\n",
    "\n",
    "    log_write.write(info+'\\n')\n",
    "\n",
    "    np.savez(perf_allRuns, acc = acc, kappa = kappa)\n",
    "\n",
    "    best_models.close()   \n",
    "    log_write.close() \n",
    "    perf_allRuns.close() \n",
    "\n",
    "def test(model, dataset_conf, results_path, allRuns = True):\n",
    "\n",
    "    log_write = open(results_path + \"/log.txt\", \"a\")\n",
    "\n",
    "    best_models = open(results_path + \"/best models.txt\", \"r\")   \n",
    "\n",
    "    n_classes = dataset_conf.get('n_classes')\n",
    "    n_sub = dataset_conf.get('n_sub')\n",
    "    data_path = dataset_conf.get('data_path')\n",
    "    isStandard = dataset_conf.get('isStandard')\n",
    "    LOSO = dataset_conf.get('LOSO')\n",
    "\n",
    "    acc_bestRun = np.zeros(n_sub)\n",
    "    kappa_bestRun = np.zeros(n_sub)  \n",
    "    cf_matrix = np.zeros([n_sub, n_classes, n_classes])\n",
    "\n",
    "    if(allRuns): \n",
    "\n",
    "        perf_allRuns = open(results_path + \"/perf_allRuns.npz\", 'rb')\n",
    "        perf_arrays = np.load(perf_allRuns)\n",
    "        acc_allRuns = perf_arrays['acc']\n",
    "        kappa_allRuns = perf_arrays['kappa']\n",
    "    \n",
    "\n",
    "    for sub in range(n_sub):\n",
    "\n",
    "        _, _, _, X_test, _, y_test_onehot = get_data(data_path, sub, LOSO, isStandard)\n",
    "\n",
    "        filepath = best_models.readline()\n",
    "        model.load_weights(results_path + filepath[:-1])\n",
    "\n",
    "        y_pred = model.predict(X_test).argmax(axis=-1)\n",
    "\n",
    "        labels = y_test_onehot.argmax(axis=-1)\n",
    "        acc_bestRun[sub] = accuracy_score(labels, y_pred)\n",
    "        kappa_bestRun[sub] = cohen_kappa_score(labels, y_pred)\n",
    "\n",
    "        cf_matrix[sub, :, :] = confusion_matrix(labels, y_pred, normalize='pred')\n",
    "        draw_confusion_matrix(cf_matrix[sub, :, :], str(sub+1), results_path)\n",
    "\n",
    "        info = 'Subject: {}   best_run: {:2}  '.format(sub+1, (filepath[filepath.find('run-')+4:filepath.find('/sub')]) )\n",
    "        info = info + 'acc: {:.4f}   kappa: {:.4f}   '.format(acc_bestRun[sub], kappa_bestRun[sub] )\n",
    "        if(allRuns): \n",
    "            info = info + 'avg_acc: {:.4f} +- {:.4f}   avg_kappa: {:.4f} +- {:.4f}'.format(\n",
    "                np.average(acc_allRuns[sub, :]), acc_allRuns[sub,:].std(),\n",
    "                np.average(kappa_allRuns[sub, :]), kappa_allRuns[sub,:].std() )\n",
    "        print(info)\n",
    "        log_write.write('\\n'+info)\n",
    "      \n",
    "    info = '\\nAverage of {} subjects - best runs:\\nAccuracy = {:.4f}   Kappa = {:.4f}\\n'.format(\n",
    "        n_sub, np.average(acc_bestRun), np.average(kappa_bestRun)) \n",
    "    if(allRuns): \n",
    "        info = info + '\\nAverage of {} subjects x {} runs (average of {} experiments):\\nAccuracy = {:.4f}   Kappa = {:.4f}'.format(\n",
    "            n_sub, acc_allRuns.shape[1], (n_sub * acc_allRuns.shape[1]),\n",
    "            np.average(acc_allRuns), np.average(kappa_allRuns)) \n",
    "    print(info)\n",
    "    log_write.write(info)\n",
    "    \n",
    "    draw_performance_barChart(n_sub, acc_bestRun, 'Accuracy')\n",
    "    draw_performance_barChart(n_sub, kappa_bestRun, 'K-score')\n",
    "\n",
    "    draw_confusion_matrix(cf_matrix.mean(0), 'All', results_path)\n",
    "\n",
    "    log_write.close() \n",
    "    \n",
    "    \n",
    "#%%\n",
    "def getModel(model_name):\n",
    "    # Select the model\n",
    "    if(model_name == 'ATCNet'):\n",
    "       \n",
    "        model = ATCNet( \n",
    "            # Dataset parameters\n",
    "            n_classes = 4, \n",
    "            in_chans = 22, \n",
    "            in_samples = 1125, \n",
    "            # Sliding window (SW) parameter\n",
    "            n_windows = 5, \n",
    "            # Attention (AT) block parameter\n",
    "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
    "            # Convolutional (CV) block parameters\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2, \n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.3,\n",
    "            # Temporal convolutional (TC) block parameters\n",
    "            tcn_depth = 2, \n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3, \n",
    "            tcn_activation='elu'\n",
    "            )     \n",
    "    elif(model_name == 'TCNet_Fusion'):\n",
    "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
    "        model = TCNet_Fusion(n_classes = 4)      \n",
    "    elif(model_name == 'EEGTCNet'):\n",
    "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
    "        model = EEGTCNet(n_classes = 4)          \n",
    "    elif(model_name == 'EEGNet'):\n",
    "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
    "        model = EEGNet_classifier(n_classes = 4) \n",
    "    elif(model_name == 'EEGNeX'):\n",
    "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
    "        model = EEGNeX_8_32(n_timesteps = 1000 , n_features = 22, n_outputs = 4)\n",
    "    else:\n",
    "        raise Exception(\"'{}' model is not supported model yet!\".format(model_name))\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "def run():\n",
    "\n",
    "    if not  os.path.exists(CFG.results_path):\n",
    "      os.makedirs(CFG.results_path)\n",
    "\n",
    "    #train()\n",
    "    dataset_conf = { 'n_classes': 4, 'n_sub': 9, 'n_channels': 22, 'data_path': CFG.data_path,\n",
    "                'isStandard': True, 'LOSO': False}\n",
    "    test(getModel(CFG.model_name), dataset_conf, CFG.results_path)    \n",
    "    \n",
    "#%%\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuI9yIV3Dfwn"
   },
   "outputs": [],
   "source": [
    "# np.load('/kaggle/working/embed_train_sub0.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
